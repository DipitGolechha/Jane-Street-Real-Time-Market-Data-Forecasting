{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84493,"databundleVersionId":9871156,"sourceType":"competition"},{"sourceId":9984093,"sourceType":"datasetVersion","datasetId":6143956},{"sourceId":175277,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":149234,"modelId":171726},{"sourceId":175280,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":149237,"modelId":171729}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:43:06.948861Z","iopub.execute_input":"2024-11-22T14:43:06.949696Z","iopub.status.idle":"2024-11-22T14:43:07.017570Z","shell.execute_reply.started":"2024-11-22T14:43:06.949663Z","shell.execute_reply":"2024-11-22T14:43:07.016674Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jane-street-real-time-market-data-forecasting/responders.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/sample_submission.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/features.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/jane_street_gateway.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/jane_street_inference_server.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/__init__.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/templates.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/relay.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/__init__.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/__init__.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport glob\n\n# Specify the directory containing the Parquet files\nparquet_folder = \"/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4\"\n\n# Use glob to find all Parquet files in the directory, including subdirectories\nparquet_files = glob.glob(os.path.join(parquet_folder, \"*.parquet\"), recursive=True)\n\n# Load all Parquet files into a single DataFrame\nfull_data = pd.concat([pd.read_parquet(file) for file in parquet_files], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:43:07.864203Z","iopub.execute_input":"2024-11-22T14:43:07.865014Z","iopub.status.idle":"2024-11-22T14:43:11.410111Z","shell.execute_reply.started":"2024-11-22T14:43:07.864977Z","shell.execute_reply":"2024-11-22T14:43:11.408933Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"full_data.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:15:41.269779Z","iopub.execute_input":"2024-11-22T14:15:41.270067Z","iopub.status.idle":"2024-11-22T14:15:43.734242Z","shell.execute_reply.started":"2024-11-22T14:15:41.270038Z","shell.execute_reply":"2024-11-22T14:15:43.733297Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         date_id  time_id  symbol_id    weight  feature_00  feature_01  \\\n2652        1190       68          0  2.888326    0.822858    0.993142   \n2653        1190       68          1  5.419313    1.066091    1.005418   \n2654        1190       68          2  1.286522    0.937979    0.660767   \n2655        1190       68          3  1.223723    1.533020    0.624954   \n2656        1190       68          4  1.949932    0.966861    0.796667   \n...          ...      ...        ...       ...         ...         ...   \n6335555     1359      967         34  1.942354    1.628178   -1.505816   \n6335556     1359      967         35  1.031354    2.238852   -1.528865   \n6335557     1359      967         36  1.175215    1.725410   -1.519612   \n6335558     1359      967         37  0.791153    1.638768   -1.036464   \n6335559     1359      967         38  2.243855    1.770795   -1.914382   \n\n         feature_02  feature_03  feature_04  feature_05  ...  feature_78  \\\n2652       0.857044    0.884959   -0.975107   -0.341566  ...   -0.326134   \n2653       0.959261    1.147944   -0.422288   -0.445365  ...   -0.338404   \n2654       1.066772    1.840652   -0.804272   -0.544113  ...    0.208513   \n2655       1.039947    1.313849   -0.787404   -0.369194  ...   -0.004096   \n2656       1.642824    0.631688   -0.494134   -0.371915  ...   -0.056480   \n...             ...         ...         ...         ...  ...         ...   \n6335555    1.773429    1.929423   -2.051412   -0.563297  ...   -0.227744   \n6335556    2.264512    1.790196   -1.943567   -0.274188  ...   -0.410193   \n6335557    1.713313    1.800693   -1.894130   -0.306129  ...   -0.463355   \n6335558    1.903985    2.469922   -1.743174   -0.403960  ...    1.368289   \n6335559    1.819340    1.660890   -1.321963   -0.408933  ...   -0.253066   \n\n         responder_0  responder_1  responder_2  responder_3  responder_4  \\\n2652       -0.185737    -0.101338    -0.013530    -0.529809    -1.459466   \n2653       -0.123878     0.010359    -0.100183    -0.449406    -0.418801   \n2654       -0.103312    -0.000147    -0.161043    -0.094505    -1.269865   \n2655       -1.170907    -0.595833    -0.456890     0.362352     1.316427   \n2656       -0.105265    -0.017680     0.623165     0.740167    -0.656922   \n...              ...          ...          ...          ...          ...   \n6335555     0.455439     0.158139     0.534798     0.201595     0.161691   \n6335556     0.827549    -0.301494     2.717165    -0.013444    -0.004061   \n6335557     1.309621     0.779653     2.355462     0.441362     0.310022   \n6335558     0.813858    -0.028485    -4.205297     1.507040     0.471927   \n6335559     4.433130     1.575209     0.365706     0.609047     0.396386   \n\n         responder_5  responder_6  responder_7  responder_8  \n2652        0.056534    -0.835184    -1.799465     0.121690  \n2653        0.022665    -0.335986    -0.518389     0.139188  \n2654        0.784683    -0.025554    -2.133145     1.558488  \n2655        0.305119     1.657557     2.210107     0.931683  \n2656        0.185397     1.026393    -1.053990    -0.146117  \n...              ...          ...          ...          ...  \n6335555     0.237516    -0.040068     0.007217    -0.114877  \n6335556     0.026838    -0.117078    -0.027254    -0.320422  \n6335557     0.478750    -0.119603    -0.059843    -0.360000  \n6335558     0.151053     2.473381     0.718089     3.575085  \n6335559     1.145184     0.777970     0.376908     1.112585  \n\n[5788312 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_id</th>\n      <th>time_id</th>\n      <th>symbol_id</th>\n      <th>weight</th>\n      <th>feature_00</th>\n      <th>feature_01</th>\n      <th>feature_02</th>\n      <th>feature_03</th>\n      <th>feature_04</th>\n      <th>feature_05</th>\n      <th>...</th>\n      <th>feature_78</th>\n      <th>responder_0</th>\n      <th>responder_1</th>\n      <th>responder_2</th>\n      <th>responder_3</th>\n      <th>responder_4</th>\n      <th>responder_5</th>\n      <th>responder_6</th>\n      <th>responder_7</th>\n      <th>responder_8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2652</th>\n      <td>1190</td>\n      <td>68</td>\n      <td>0</td>\n      <td>2.888326</td>\n      <td>0.822858</td>\n      <td>0.993142</td>\n      <td>0.857044</td>\n      <td>0.884959</td>\n      <td>-0.975107</td>\n      <td>-0.341566</td>\n      <td>...</td>\n      <td>-0.326134</td>\n      <td>-0.185737</td>\n      <td>-0.101338</td>\n      <td>-0.013530</td>\n      <td>-0.529809</td>\n      <td>-1.459466</td>\n      <td>0.056534</td>\n      <td>-0.835184</td>\n      <td>-1.799465</td>\n      <td>0.121690</td>\n    </tr>\n    <tr>\n      <th>2653</th>\n      <td>1190</td>\n      <td>68</td>\n      <td>1</td>\n      <td>5.419313</td>\n      <td>1.066091</td>\n      <td>1.005418</td>\n      <td>0.959261</td>\n      <td>1.147944</td>\n      <td>-0.422288</td>\n      <td>-0.445365</td>\n      <td>...</td>\n      <td>-0.338404</td>\n      <td>-0.123878</td>\n      <td>0.010359</td>\n      <td>-0.100183</td>\n      <td>-0.449406</td>\n      <td>-0.418801</td>\n      <td>0.022665</td>\n      <td>-0.335986</td>\n      <td>-0.518389</td>\n      <td>0.139188</td>\n    </tr>\n    <tr>\n      <th>2654</th>\n      <td>1190</td>\n      <td>68</td>\n      <td>2</td>\n      <td>1.286522</td>\n      <td>0.937979</td>\n      <td>0.660767</td>\n      <td>1.066772</td>\n      <td>1.840652</td>\n      <td>-0.804272</td>\n      <td>-0.544113</td>\n      <td>...</td>\n      <td>0.208513</td>\n      <td>-0.103312</td>\n      <td>-0.000147</td>\n      <td>-0.161043</td>\n      <td>-0.094505</td>\n      <td>-1.269865</td>\n      <td>0.784683</td>\n      <td>-0.025554</td>\n      <td>-2.133145</td>\n      <td>1.558488</td>\n    </tr>\n    <tr>\n      <th>2655</th>\n      <td>1190</td>\n      <td>68</td>\n      <td>3</td>\n      <td>1.223723</td>\n      <td>1.533020</td>\n      <td>0.624954</td>\n      <td>1.039947</td>\n      <td>1.313849</td>\n      <td>-0.787404</td>\n      <td>-0.369194</td>\n      <td>...</td>\n      <td>-0.004096</td>\n      <td>-1.170907</td>\n      <td>-0.595833</td>\n      <td>-0.456890</td>\n      <td>0.362352</td>\n      <td>1.316427</td>\n      <td>0.305119</td>\n      <td>1.657557</td>\n      <td>2.210107</td>\n      <td>0.931683</td>\n    </tr>\n    <tr>\n      <th>2656</th>\n      <td>1190</td>\n      <td>68</td>\n      <td>4</td>\n      <td>1.949932</td>\n      <td>0.966861</td>\n      <td>0.796667</td>\n      <td>1.642824</td>\n      <td>0.631688</td>\n      <td>-0.494134</td>\n      <td>-0.371915</td>\n      <td>...</td>\n      <td>-0.056480</td>\n      <td>-0.105265</td>\n      <td>-0.017680</td>\n      <td>0.623165</td>\n      <td>0.740167</td>\n      <td>-0.656922</td>\n      <td>0.185397</td>\n      <td>1.026393</td>\n      <td>-1.053990</td>\n      <td>-0.146117</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6335555</th>\n      <td>1359</td>\n      <td>967</td>\n      <td>34</td>\n      <td>1.942354</td>\n      <td>1.628178</td>\n      <td>-1.505816</td>\n      <td>1.773429</td>\n      <td>1.929423</td>\n      <td>-2.051412</td>\n      <td>-0.563297</td>\n      <td>...</td>\n      <td>-0.227744</td>\n      <td>0.455439</td>\n      <td>0.158139</td>\n      <td>0.534798</td>\n      <td>0.201595</td>\n      <td>0.161691</td>\n      <td>0.237516</td>\n      <td>-0.040068</td>\n      <td>0.007217</td>\n      <td>-0.114877</td>\n    </tr>\n    <tr>\n      <th>6335556</th>\n      <td>1359</td>\n      <td>967</td>\n      <td>35</td>\n      <td>1.031354</td>\n      <td>2.238852</td>\n      <td>-1.528865</td>\n      <td>2.264512</td>\n      <td>1.790196</td>\n      <td>-1.943567</td>\n      <td>-0.274188</td>\n      <td>...</td>\n      <td>-0.410193</td>\n      <td>0.827549</td>\n      <td>-0.301494</td>\n      <td>2.717165</td>\n      <td>-0.013444</td>\n      <td>-0.004061</td>\n      <td>0.026838</td>\n      <td>-0.117078</td>\n      <td>-0.027254</td>\n      <td>-0.320422</td>\n    </tr>\n    <tr>\n      <th>6335557</th>\n      <td>1359</td>\n      <td>967</td>\n      <td>36</td>\n      <td>1.175215</td>\n      <td>1.725410</td>\n      <td>-1.519612</td>\n      <td>1.713313</td>\n      <td>1.800693</td>\n      <td>-1.894130</td>\n      <td>-0.306129</td>\n      <td>...</td>\n      <td>-0.463355</td>\n      <td>1.309621</td>\n      <td>0.779653</td>\n      <td>2.355462</td>\n      <td>0.441362</td>\n      <td>0.310022</td>\n      <td>0.478750</td>\n      <td>-0.119603</td>\n      <td>-0.059843</td>\n      <td>-0.360000</td>\n    </tr>\n    <tr>\n      <th>6335558</th>\n      <td>1359</td>\n      <td>967</td>\n      <td>37</td>\n      <td>0.791153</td>\n      <td>1.638768</td>\n      <td>-1.036464</td>\n      <td>1.903985</td>\n      <td>2.469922</td>\n      <td>-1.743174</td>\n      <td>-0.403960</td>\n      <td>...</td>\n      <td>1.368289</td>\n      <td>0.813858</td>\n      <td>-0.028485</td>\n      <td>-4.205297</td>\n      <td>1.507040</td>\n      <td>0.471927</td>\n      <td>0.151053</td>\n      <td>2.473381</td>\n      <td>0.718089</td>\n      <td>3.575085</td>\n    </tr>\n    <tr>\n      <th>6335559</th>\n      <td>1359</td>\n      <td>967</td>\n      <td>38</td>\n      <td>2.243855</td>\n      <td>1.770795</td>\n      <td>-1.914382</td>\n      <td>1.819340</td>\n      <td>1.660890</td>\n      <td>-1.321963</td>\n      <td>-0.408933</td>\n      <td>...</td>\n      <td>-0.253066</td>\n      <td>4.433130</td>\n      <td>1.575209</td>\n      <td>0.365706</td>\n      <td>0.609047</td>\n      <td>0.396386</td>\n      <td>1.145184</td>\n      <td>0.777970</td>\n      <td>0.376908</td>\n      <td>1.112585</td>\n    </tr>\n  </tbody>\n</table>\n<p>5788312 rows × 92 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df_top_1000_per_symbol = (\n    full_data.groupby('symbol_id')\n      .apply(lambda group: group.head(5000))\n      .reset_index(drop=True)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:16:51.491829Z","iopub.execute_input":"2024-11-22T14:16:51.492797Z","iopub.status.idle":"2024-11-22T14:16:55.307090Z","shell.execute_reply.started":"2024-11-22T14:16:51.492753Z","shell.execute_reply":"2024-11-22T14:16:55.306007Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/510963304.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  .apply(lambda group: group.head(5000))\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df_top_1000_per_symbol.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:16:55.308931Z","iopub.execute_input":"2024-11-22T14:16:55.309641Z","iopub.status.idle":"2024-11-22T14:16:55.382768Z","shell.execute_reply.started":"2024-11-22T14:16:55.309593Z","shell.execute_reply":"2024-11-22T14:16:55.381951Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"        date_id  time_id  symbol_id    weight  feature_00  feature_01  \\\n68         1190       68          0  2.888326    0.822858    0.993142   \n69         1190       69          0  2.888326    1.094897    1.348959   \n70         1190       70          0  2.888326    1.225544    1.278140   \n71         1190       71          0  2.888326    0.873633    0.911349   \n72         1190       72          0  2.888326    0.799049    0.143772   \n...         ...      ...        ...       ...         ...         ...   \n194995     1195      155         38  2.341381    1.082160    2.340072   \n194996     1195      156         38  2.341381    1.556517    3.213494   \n194997     1195      157         38  2.341381    1.220852    3.058960   \n194998     1195      158         38  2.341381    1.212757    3.399373   \n194999     1195      159         38  2.341381    1.400576    3.178139   \n\n        feature_02  feature_03  feature_04  feature_05  ...  feature_78  \\\n68        0.857044    0.884959   -0.975107   -0.341566  ...   -0.326134   \n69        0.962777    1.151421    0.565633   -0.467618  ...   -0.264703   \n70        0.925267    0.842146   -0.564982   -0.504759  ...   -0.332486   \n71        1.108208    0.966368   -0.407035   -0.744210  ...   -0.293733   \n72        0.327584    0.670475   -0.060378   -0.304076  ...   -0.220144   \n...            ...         ...         ...         ...  ...         ...   \n194995    1.049053    0.792264    2.273375    1.740894  ...    3.654527   \n194996    0.665002    0.829738    1.688604    3.617028  ...    2.226776   \n194997    1.056065    0.685058    1.750810    2.010409  ...    5.278824   \n194998    1.037152    0.772950    1.597517    2.808936  ...    2.785875   \n194999    0.737320    1.139716    1.393347    2.395722  ...    3.576414   \n\n        responder_0  responder_1  responder_2  responder_3  responder_4  \\\n68        -0.185737    -0.101338    -0.013530    -0.529809    -1.459466   \n69        -0.193914    -0.080728     0.246754    -0.487381    -1.257926   \n70        -0.116445    -0.158977     0.363354    -1.012045    -0.971144   \n71        -0.015851    -0.106900     0.345716    -0.830448    -0.940993   \n72        -0.105784    -0.135022     0.222614    -0.544093    -1.452412   \n...             ...          ...          ...          ...          ...   \n194995    -0.293005    -0.168925    -0.370438    -0.443664    -0.553788   \n194996    -0.038351    -0.081051    -0.244196    -0.120059    -0.620428   \n194997    -0.790689    -0.279192    -1.428786    -0.720165    -0.531759   \n194998     0.004962    -0.059438     0.408630    -0.508299    -0.806200   \n194999    -0.206226    -0.053458    -0.145534    -0.735913    -1.030561   \n\n        responder_5  responder_6  responder_7  responder_8  \n68         0.056534    -0.835184    -1.799465     0.121690  \n69         0.311781    -1.164844    -1.414613     0.109678  \n70         0.613950    -0.809712    -1.072013     0.590317  \n71         0.472611    -0.359095    -1.343737     0.376321  \n72         0.284815    -0.944981    -0.863599     0.317497  \n...             ...          ...          ...          ...  \n194995     0.203772    -0.300322    -0.632161     0.846210  \n194996     0.349025    -0.068648    -0.691732     0.586286  \n194997    -0.817945    -0.377834    -0.538160     0.539742  \n194998    -1.170922    -0.475770    -1.029748    -2.741258  \n194999    -1.175721    -0.483635    -0.739716    -1.998718  \n\n[179001 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_id</th>\n      <th>time_id</th>\n      <th>symbol_id</th>\n      <th>weight</th>\n      <th>feature_00</th>\n      <th>feature_01</th>\n      <th>feature_02</th>\n      <th>feature_03</th>\n      <th>feature_04</th>\n      <th>feature_05</th>\n      <th>...</th>\n      <th>feature_78</th>\n      <th>responder_0</th>\n      <th>responder_1</th>\n      <th>responder_2</th>\n      <th>responder_3</th>\n      <th>responder_4</th>\n      <th>responder_5</th>\n      <th>responder_6</th>\n      <th>responder_7</th>\n      <th>responder_8</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>68</th>\n      <td>1190</td>\n      <td>68</td>\n      <td>0</td>\n      <td>2.888326</td>\n      <td>0.822858</td>\n      <td>0.993142</td>\n      <td>0.857044</td>\n      <td>0.884959</td>\n      <td>-0.975107</td>\n      <td>-0.341566</td>\n      <td>...</td>\n      <td>-0.326134</td>\n      <td>-0.185737</td>\n      <td>-0.101338</td>\n      <td>-0.013530</td>\n      <td>-0.529809</td>\n      <td>-1.459466</td>\n      <td>0.056534</td>\n      <td>-0.835184</td>\n      <td>-1.799465</td>\n      <td>0.121690</td>\n    </tr>\n    <tr>\n      <th>69</th>\n      <td>1190</td>\n      <td>69</td>\n      <td>0</td>\n      <td>2.888326</td>\n      <td>1.094897</td>\n      <td>1.348959</td>\n      <td>0.962777</td>\n      <td>1.151421</td>\n      <td>0.565633</td>\n      <td>-0.467618</td>\n      <td>...</td>\n      <td>-0.264703</td>\n      <td>-0.193914</td>\n      <td>-0.080728</td>\n      <td>0.246754</td>\n      <td>-0.487381</td>\n      <td>-1.257926</td>\n      <td>0.311781</td>\n      <td>-1.164844</td>\n      <td>-1.414613</td>\n      <td>0.109678</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>1190</td>\n      <td>70</td>\n      <td>0</td>\n      <td>2.888326</td>\n      <td>1.225544</td>\n      <td>1.278140</td>\n      <td>0.925267</td>\n      <td>0.842146</td>\n      <td>-0.564982</td>\n      <td>-0.504759</td>\n      <td>...</td>\n      <td>-0.332486</td>\n      <td>-0.116445</td>\n      <td>-0.158977</td>\n      <td>0.363354</td>\n      <td>-1.012045</td>\n      <td>-0.971144</td>\n      <td>0.613950</td>\n      <td>-0.809712</td>\n      <td>-1.072013</td>\n      <td>0.590317</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>1190</td>\n      <td>71</td>\n      <td>0</td>\n      <td>2.888326</td>\n      <td>0.873633</td>\n      <td>0.911349</td>\n      <td>1.108208</td>\n      <td>0.966368</td>\n      <td>-0.407035</td>\n      <td>-0.744210</td>\n      <td>...</td>\n      <td>-0.293733</td>\n      <td>-0.015851</td>\n      <td>-0.106900</td>\n      <td>0.345716</td>\n      <td>-0.830448</td>\n      <td>-0.940993</td>\n      <td>0.472611</td>\n      <td>-0.359095</td>\n      <td>-1.343737</td>\n      <td>0.376321</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>1190</td>\n      <td>72</td>\n      <td>0</td>\n      <td>2.888326</td>\n      <td>0.799049</td>\n      <td>0.143772</td>\n      <td>0.327584</td>\n      <td>0.670475</td>\n      <td>-0.060378</td>\n      <td>-0.304076</td>\n      <td>...</td>\n      <td>-0.220144</td>\n      <td>-0.105784</td>\n      <td>-0.135022</td>\n      <td>0.222614</td>\n      <td>-0.544093</td>\n      <td>-1.452412</td>\n      <td>0.284815</td>\n      <td>-0.944981</td>\n      <td>-0.863599</td>\n      <td>0.317497</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>194995</th>\n      <td>1195</td>\n      <td>155</td>\n      <td>38</td>\n      <td>2.341381</td>\n      <td>1.082160</td>\n      <td>2.340072</td>\n      <td>1.049053</td>\n      <td>0.792264</td>\n      <td>2.273375</td>\n      <td>1.740894</td>\n      <td>...</td>\n      <td>3.654527</td>\n      <td>-0.293005</td>\n      <td>-0.168925</td>\n      <td>-0.370438</td>\n      <td>-0.443664</td>\n      <td>-0.553788</td>\n      <td>0.203772</td>\n      <td>-0.300322</td>\n      <td>-0.632161</td>\n      <td>0.846210</td>\n    </tr>\n    <tr>\n      <th>194996</th>\n      <td>1195</td>\n      <td>156</td>\n      <td>38</td>\n      <td>2.341381</td>\n      <td>1.556517</td>\n      <td>3.213494</td>\n      <td>0.665002</td>\n      <td>0.829738</td>\n      <td>1.688604</td>\n      <td>3.617028</td>\n      <td>...</td>\n      <td>2.226776</td>\n      <td>-0.038351</td>\n      <td>-0.081051</td>\n      <td>-0.244196</td>\n      <td>-0.120059</td>\n      <td>-0.620428</td>\n      <td>0.349025</td>\n      <td>-0.068648</td>\n      <td>-0.691732</td>\n      <td>0.586286</td>\n    </tr>\n    <tr>\n      <th>194997</th>\n      <td>1195</td>\n      <td>157</td>\n      <td>38</td>\n      <td>2.341381</td>\n      <td>1.220852</td>\n      <td>3.058960</td>\n      <td>1.056065</td>\n      <td>0.685058</td>\n      <td>1.750810</td>\n      <td>2.010409</td>\n      <td>...</td>\n      <td>5.278824</td>\n      <td>-0.790689</td>\n      <td>-0.279192</td>\n      <td>-1.428786</td>\n      <td>-0.720165</td>\n      <td>-0.531759</td>\n      <td>-0.817945</td>\n      <td>-0.377834</td>\n      <td>-0.538160</td>\n      <td>0.539742</td>\n    </tr>\n    <tr>\n      <th>194998</th>\n      <td>1195</td>\n      <td>158</td>\n      <td>38</td>\n      <td>2.341381</td>\n      <td>1.212757</td>\n      <td>3.399373</td>\n      <td>1.037152</td>\n      <td>0.772950</td>\n      <td>1.597517</td>\n      <td>2.808936</td>\n      <td>...</td>\n      <td>2.785875</td>\n      <td>0.004962</td>\n      <td>-0.059438</td>\n      <td>0.408630</td>\n      <td>-0.508299</td>\n      <td>-0.806200</td>\n      <td>-1.170922</td>\n      <td>-0.475770</td>\n      <td>-1.029748</td>\n      <td>-2.741258</td>\n    </tr>\n    <tr>\n      <th>194999</th>\n      <td>1195</td>\n      <td>159</td>\n      <td>38</td>\n      <td>2.341381</td>\n      <td>1.400576</td>\n      <td>3.178139</td>\n      <td>0.737320</td>\n      <td>1.139716</td>\n      <td>1.393347</td>\n      <td>2.395722</td>\n      <td>...</td>\n      <td>3.576414</td>\n      <td>-0.206226</td>\n      <td>-0.053458</td>\n      <td>-0.145534</td>\n      <td>-0.735913</td>\n      <td>-1.030561</td>\n      <td>-1.175721</td>\n      <td>-0.483635</td>\n      <td>-0.739716</td>\n      <td>-1.998718</td>\n    </tr>\n  </tbody>\n</table>\n<p>179001 rows × 92 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df = df_top_1000_per_symbol.dropna()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:16:55.383965Z","iopub.execute_input":"2024-11-22T14:16:55.384333Z","iopub.status.idle":"2024-11-22T14:16:55.431323Z","shell.execute_reply.started":"2024-11-22T14:16:55.384290Z","shell.execute_reply":"2024-11-22T14:16:55.430564Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"# CPU Implementation (Random Forest)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\n\n# Assuming `features_*` are all columns starting with \"feature_\"\nfeature_columns = [col for col in df.columns if col.startswith('feature_')]\ntarget_column = 'responder_6'\n\n# Extract features and target\nX = df[feature_columns]\ny = df[target_column]\n\n# Handle missing values (fill with mean as an example)\nX = X.fillna(X.mean())\ny = y.fillna(y.mean())\n\n# Normalize features (optional)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Set up cross-validation\nn_splits = 5  # Number of folds\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# Initialize lists to store results\nmse_scores = []\nr2_scores = []\n\nfold = 1\nfor train_index, test_index in kf.split(X_scaled):\n    print(f\"\\nStarting Fold {fold}/{n_splits}\")\n\n    # Split data into training and validation sets for this fold\n    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Train a Random Forest Regressor\n    model = RandomForestRegressor(random_state=42, n_estimators=100, n_jobs=-1)\n    model.fit(X_train, y_train)\n\n    # Predict on the validation set\n    y_pred = model.predict(X_test)\n\n    # Evaluate the model\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    print(f\"Fold {fold} Mean Squared Error: {mse:.4f}\")\n    print(f\"Fold {fold} R-squared: {r2:.4f}\")\n\n    # Store the scores\n    mse_scores.append(mse)\n    r2_scores.append(r2)\n\n    fold += 1\n\n# Calculate and print the average scores\navg_mse = sum(mse_scores) / n_splits\navg_r2 = sum(r2_scores) / n_splits\n\nprint(\"\\nCross-Validation Results:\")\nprint(f\"Average Mean Squared Error over {n_splits} folds: {avg_mse:.4f}\")\nprint(f\"Average R-squared over {n_splits} folds: {avg_r2:.4f}\")\n\n# Optionally, you can train on the entire dataset\n# Train a final model on the full dataset\nfinal_model = RandomForestRegressor(random_state=42, n_estimators=100, n_jobs=-1)\nfinal_model.fit(X_scaled, y)\n\n# Feature Importance from the final model\nimportances = final_model.feature_importances_\nfeature_importance = pd.DataFrame({'Feature': feature_columns, 'Importance': importances})\nfeature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nTop 10 Important Features:\")\nprint(feature_importance.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:10:52.583119Z","iopub.execute_input":"2024-11-22T14:10:52.583484Z","iopub.status.idle":"2024-11-22T14:14:41.946472Z","shell.execute_reply.started":"2024-11-22T14:10:52.583455Z","shell.execute_reply":"2024-11-22T14:14:41.942084Z"}},"outputs":[{"name":"stdout","text":"\nStarting Fold 1/5\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Train a Random Forest Regressor\u001b[39;00m\n\u001b[1;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Predict on the validation set\u001b[39;00m\n\u001b[1;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    465\u001b[0m ]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"# GPU Implementation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb\nimport numpy as np\n\ndf = full_data\n\n# Assuming `features_*` are all columns starting with \"feature_\"\nfeature_columns = [col for col in df.columns if col.startswith('feature_')]\ntarget_column = 'responder_6'\n\n# Extract features and target\nX = df[feature_columns]\ny = df[target_column]\n\n# Handle missing values (XGBoost can handle missing values natively)\n# If you still want to fill them, you can uncomment the next lines\n# X = X.fillna(X.mean())\n# y = y.fillna(y.mean())\n\n# Normalize features (optional for tree-based models)\n# Scaling is not necessary for XGBoost, but you can do it if you prefer\n# scaler = StandardScaler()\n# X_scaled = scaler.fit_transform(X)\nX_scaled = X.values  # Use values directly without scaling\n\n# Set up cross-validation\nn_splits = 5  # Number of folds\nkf = KFold(n_splits=n_splits, shuffle=True, random_state=4)\n\n# Initialize lists to store results\nmse_scores = []\nr2_scores = []\n\nfold = 1\nfor train_index, test_index in kf.split(X_scaled):\n    print(f\"\\nStarting Fold {fold}/{n_splits}\")\n\n    # Split data into training and validation sets for this fold\n    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # Create DMatrix for XGBoost\n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dtest = xgb.DMatrix(X_test, label=y_test)\n\n    # Set up parameters for XGBoost\n    params = {\n        'objective': 'reg:squarederror',\n        'tree_method': 'gpu_hist',  # Use GPU acceleration\n        'predictor': 'gpu_predictor',\n        'eval_metric': 'rmse',\n        'verbosity': 1,\n        'seed': 4,\n        'max_depth': 8,\n        'learning_rate': 0.08,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8\n    }\n\n    # Train the model\n    evals_result = {}\n    model = xgb.train(\n        params,\n        dtrain,\n        num_boost_round=1000,\n        evals=[(dtrain, 'train'), (dtest, 'eval')],\n        early_stopping_rounds=50,\n        evals_result=evals_result,\n        verbose_eval=10\n    )\n\n    # Predict on the validation set\n    y_pred = model.predict(dtest)\n\n    # Evaluate the model\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n\n    print(f\"Fold {fold} Mean Squared Error: {mse:.4f}\")\n    print(f\"Fold {fold} R-squared: {r2:.4f}\")\n\n    # Store the scores\n    mse_scores.append(mse)\n    r2_scores.append(r2)\n\n    fold += 1\n\n# Calculate and print the average scores\navg_mse = np.mean(mse_scores)\navg_r2 = np.mean(r2_scores)\n\nprint(\"\\nCross-Validation Results:\")\nprint(f\"Average Mean Squared Error over {n_splits} folds: {avg_mse:.4f}\")\nprint(f\"Average R-squared over {n_splits} folds: {avg_r2:.4f}\")\n\n# Optionally, you can train on the entire dataset\n# Create DMatrix for the full dataset\ndtrain_full = xgb.DMatrix(X_scaled, label=y)\n\n# Train a final model on the full dataset\nfinal_model = xgb.train(\n    params,\n    dtrain_full,\n    num_boost_round=model.best_iteration\n)\n\n# Feature Importance from the final model\nimportances = final_model.get_score(importance_type='gain')\nimportances = {int(k[1:]): v for k, v in importances.items()}  # Convert feature names to indices\n\n# Create a DataFrame for feature importance\nfeature_importance = pd.DataFrame({\n    'Feature': [feature_columns[i] for i in importances.keys()],\n    'Importance': list(importances.values())\n})\n\nfeature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n\nprint(\"\\nTop 10 Important Features:\")\nprint(feature_importance.head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T14:43:31.752854Z","iopub.execute_input":"2024-11-22T14:43:31.753273Z"}},"outputs":[{"name":"stdout","text":"\nStarting Fold 1/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:43:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:43:41] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"predictor\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[0]\ttrain-rmse:0.94221\teval-rmse:0.94214\n[10]\ttrain-rmse:0.93072\teval-rmse:0.93262\n[20]\ttrain-rmse:0.92379\teval-rmse:0.92728\n[30]\ttrain-rmse:0.91843\teval-rmse:0.92328\n[40]\ttrain-rmse:0.91397\teval-rmse:0.92010\n[50]\ttrain-rmse:0.90941\teval-rmse:0.91671\n[60]\ttrain-rmse:0.90564\teval-rmse:0.91397\n[70]\ttrain-rmse:0.90220\teval-rmse:0.91162\n[80]\ttrain-rmse:0.89877\teval-rmse:0.90910\n[90]\ttrain-rmse:0.89499\teval-rmse:0.90630\n[100]\ttrain-rmse:0.89207\teval-rmse:0.90426\n[110]\ttrain-rmse:0.88843\teval-rmse:0.90148\n[120]\ttrain-rmse:0.88508\teval-rmse:0.89894\n[130]\ttrain-rmse:0.88202\teval-rmse:0.89666\n[140]\ttrain-rmse:0.87921\teval-rmse:0.89468\n[150]\ttrain-rmse:0.87628\teval-rmse:0.89252\n[160]\ttrain-rmse:0.87369\teval-rmse:0.89063\n[170]\ttrain-rmse:0.87104\teval-rmse:0.88866\n[180]\ttrain-rmse:0.86852\teval-rmse:0.88675\n[190]\ttrain-rmse:0.86611\teval-rmse:0.88501\n[200]\ttrain-rmse:0.86345\teval-rmse:0.88319\n[210]\ttrain-rmse:0.86086\teval-rmse:0.88133\n[220]\ttrain-rmse:0.85821\teval-rmse:0.87928\n[230]\ttrain-rmse:0.85562\teval-rmse:0.87732\n[240]\ttrain-rmse:0.85330\teval-rmse:0.87560\n[250]\ttrain-rmse:0.85097\teval-rmse:0.87386\n[260]\ttrain-rmse:0.84856\teval-rmse:0.87199\n[270]\ttrain-rmse:0.84641\teval-rmse:0.87057\n[280]\ttrain-rmse:0.84429\teval-rmse:0.86902\n[290]\ttrain-rmse:0.84225\teval-rmse:0.86757\n[300]\ttrain-rmse:0.84011\teval-rmse:0.86602\n[310]\ttrain-rmse:0.83783\teval-rmse:0.86426\n[320]\ttrain-rmse:0.83585\teval-rmse:0.86286\n[330]\ttrain-rmse:0.83371\teval-rmse:0.86130\n[340]\ttrain-rmse:0.83185\teval-rmse:0.85994\n[350]\ttrain-rmse:0.82969\teval-rmse:0.85834\n[360]\ttrain-rmse:0.82760\teval-rmse:0.85681\n[370]\ttrain-rmse:0.82567\teval-rmse:0.85537\n[380]\ttrain-rmse:0.82396\teval-rmse:0.85412\n[390]\ttrain-rmse:0.82225\teval-rmse:0.85289\n[400]\ttrain-rmse:0.82045\teval-rmse:0.85154\n[410]\ttrain-rmse:0.81841\teval-rmse:0.84998\n[420]\ttrain-rmse:0.81672\teval-rmse:0.84871\n[430]\ttrain-rmse:0.81468\teval-rmse:0.84715\n[440]\ttrain-rmse:0.81297\teval-rmse:0.84591\n[450]\ttrain-rmse:0.81151\teval-rmse:0.84486\n[460]\ttrain-rmse:0.80973\teval-rmse:0.84358\n[470]\ttrain-rmse:0.80806\teval-rmse:0.84242\n[480]\ttrain-rmse:0.80640\teval-rmse:0.84120\n[490]\ttrain-rmse:0.80490\teval-rmse:0.84020\n[500]\ttrain-rmse:0.80332\teval-rmse:0.83911\n[510]\ttrain-rmse:0.80197\teval-rmse:0.83817\n[520]\ttrain-rmse:0.80031\teval-rmse:0.83701\n[530]\ttrain-rmse:0.79887\teval-rmse:0.83603\n[540]\ttrain-rmse:0.79744\teval-rmse:0.83501\n[550]\ttrain-rmse:0.79583\teval-rmse:0.83385\n[560]\ttrain-rmse:0.79434\teval-rmse:0.83280\n[570]\ttrain-rmse:0.79283\teval-rmse:0.83176\n[580]\ttrain-rmse:0.79135\teval-rmse:0.83071\n[590]\ttrain-rmse:0.78988\teval-rmse:0.82962\n[600]\ttrain-rmse:0.78828\teval-rmse:0.82846\n[610]\ttrain-rmse:0.78675\teval-rmse:0.82735\n[620]\ttrain-rmse:0.78516\teval-rmse:0.82621\n[630]\ttrain-rmse:0.78373\teval-rmse:0.82521\n[640]\ttrain-rmse:0.78231\teval-rmse:0.82426\n[650]\ttrain-rmse:0.78079\teval-rmse:0.82317\n[660]\ttrain-rmse:0.77940\teval-rmse:0.82216\n[670]\ttrain-rmse:0.77807\teval-rmse:0.82123\n[680]\ttrain-rmse:0.77683\teval-rmse:0.82039\n[690]\ttrain-rmse:0.77571\teval-rmse:0.81967\n[700]\ttrain-rmse:0.77433\teval-rmse:0.81872\n[710]\ttrain-rmse:0.77300\teval-rmse:0.81780\n[720]\ttrain-rmse:0.77167\teval-rmse:0.81683\n[730]\ttrain-rmse:0.77052\teval-rmse:0.81604\n[740]\ttrain-rmse:0.76938\teval-rmse:0.81529\n[750]\ttrain-rmse:0.76813\teval-rmse:0.81438\n[760]\ttrain-rmse:0.76695\teval-rmse:0.81359\n[770]\ttrain-rmse:0.76569\teval-rmse:0.81276\n[780]\ttrain-rmse:0.76446\teval-rmse:0.81193\n[790]\ttrain-rmse:0.76319\teval-rmse:0.81104\n[800]\ttrain-rmse:0.76209\teval-rmse:0.81033\n[810]\ttrain-rmse:0.76074\teval-rmse:0.80933\n[820]\ttrain-rmse:0.75957\teval-rmse:0.80858\n[830]\ttrain-rmse:0.75839\teval-rmse:0.80778\n[840]\ttrain-rmse:0.75715\teval-rmse:0.80692\n[850]\ttrain-rmse:0.75605\teval-rmse:0.80616\n[860]\ttrain-rmse:0.75488\teval-rmse:0.80539\n[870]\ttrain-rmse:0.75357\teval-rmse:0.80448\n[880]\ttrain-rmse:0.75242\teval-rmse:0.80365\n[890]\ttrain-rmse:0.75122\teval-rmse:0.80279\n[900]\ttrain-rmse:0.75003\teval-rmse:0.80198\n[910]\ttrain-rmse:0.74876\teval-rmse:0.80107\n[920]\ttrain-rmse:0.74758\teval-rmse:0.80028\n[930]\ttrain-rmse:0.74632\teval-rmse:0.79936\n[940]\ttrain-rmse:0.74510\teval-rmse:0.79854\n[950]\ttrain-rmse:0.74398\teval-rmse:0.79777\n[960]\ttrain-rmse:0.74282\teval-rmse:0.79694\n[970]\ttrain-rmse:0.74170\teval-rmse:0.79619\n[980]\ttrain-rmse:0.74074\teval-rmse:0.79559\n[990]\ttrain-rmse:0.73964\teval-rmse:0.79484\n[999]\ttrain-rmse:0.73859\teval-rmse:0.79412\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:46:05] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Fold 1 Mean Squared Error: 0.6306\nFold 1 R-squared: 0.2917\n\nStarting Fold 2/5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:46:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n\n    E.g. tree_method = \"hist\", device = \"cuda\"\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [14:46:12] WARNING: /workspace/src/learner.cc:742: \nParameters: { \"predictor\" } are not used.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[0]\ttrain-rmse:0.94232\teval-rmse:0.94205\n[10]\ttrain-rmse:0.93080\teval-rmse:0.93249\n[20]\ttrain-rmse:0.92389\teval-rmse:0.92721\n[30]\ttrain-rmse:0.91866\teval-rmse:0.92348\n[40]\ttrain-rmse:0.91389\teval-rmse:0.91988\n[50]\ttrain-rmse:0.90990\teval-rmse:0.91701\n[60]\ttrain-rmse:0.90581\teval-rmse:0.91396\n[70]\ttrain-rmse:0.90188\teval-rmse:0.91102\n[80]\ttrain-rmse:0.89813\teval-rmse:0.90823\n[90]\ttrain-rmse:0.89479\teval-rmse:0.90578\n[100]\ttrain-rmse:0.89168\teval-rmse:0.90359\n[110]\ttrain-rmse:0.88850\teval-rmse:0.90116\n[120]\ttrain-rmse:0.88558\teval-rmse:0.89916\n[130]\ttrain-rmse:0.88315\teval-rmse:0.89754\n[140]\ttrain-rmse:0.88005\teval-rmse:0.89525\n[150]\ttrain-rmse:0.87696\teval-rmse:0.89292\n[160]\ttrain-rmse:0.87368\teval-rmse:0.89045\n[170]\ttrain-rmse:0.87030\teval-rmse:0.88782\n[180]\ttrain-rmse:0.86809\teval-rmse:0.88629\n[190]\ttrain-rmse:0.86558\teval-rmse:0.88449\n[200]\ttrain-rmse:0.86308\teval-rmse:0.88285\n[210]\ttrain-rmse:0.86057\teval-rmse:0.88100\n[220]\ttrain-rmse:0.85798\teval-rmse:0.87909\n[230]\ttrain-rmse:0.85550\teval-rmse:0.87729\n[240]\ttrain-rmse:0.85301\teval-rmse:0.87547\n[250]\ttrain-rmse:0.85054\teval-rmse:0.87370\n[260]\ttrain-rmse:0.84821\teval-rmse:0.87205\n[270]\ttrain-rmse:0.84597\teval-rmse:0.87041\n[280]\ttrain-rmse:0.84398\teval-rmse:0.86903\n[290]\ttrain-rmse:0.84187\teval-rmse:0.86755\n[300]\ttrain-rmse:0.83988\teval-rmse:0.86620\n[310]\ttrain-rmse:0.83781\teval-rmse:0.86469\n[320]\ttrain-rmse:0.83547\teval-rmse:0.86290\n[330]\ttrain-rmse:0.83365\teval-rmse:0.86161\n[340]\ttrain-rmse:0.83165\teval-rmse:0.86019\n[350]\ttrain-rmse:0.83001\teval-rmse:0.85907\n[360]\ttrain-rmse:0.82824\teval-rmse:0.85791\n[370]\ttrain-rmse:0.82615\teval-rmse:0.85638\n[380]\ttrain-rmse:0.82417\teval-rmse:0.85492\n[390]\ttrain-rmse:0.82203\teval-rmse:0.85327\n[400]\ttrain-rmse:0.82011\teval-rmse:0.85187\n[410]\ttrain-rmse:0.81830\teval-rmse:0.85054\n[420]\ttrain-rmse:0.81641\teval-rmse:0.84917\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nepochs = len(evals_result['train']['rmse'])\nx_axis = range(0, epochs)\n\n# Plot RMSE\nplt.figure()\nplt.plot(x_axis, evals_result['train']['rmse'], label='Train')\nplt.plot(x_axis, evals_result['eval']['rmse'], label='Test')\nplt.legend()\nplt.ylabel('RMSE')\nplt.title('XGBoost RMSE')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model\nfinal_model.save_model('xgb_model_3.json')\n\n# Load model\nloaded_model = xgb.Booster()\nloaded_model.load_model('xgb_model_3.json')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submitting ","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:34:21.481739Z","iopub.execute_input":"2024-11-22T15:34:21.482130Z","iopub.status.idle":"2024-11-22T15:34:21.520274Z","shell.execute_reply.started":"2024-11-22T15:34:21.482087Z","shell.execute_reply":"2024-11-22T15:34:21.519468Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/jane-street-real-time-market-data-forecasting/responders.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/sample_submission.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/features.csv\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=4/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=5/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=6/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=3/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=1/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=8/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=2/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=7/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/train.parquet/partition_id=9/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet/date_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet/date_id=0/part-0.parquet\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/jane_street_gateway.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/jane_street_inference_server.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/__init__.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/templates.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/base_gateway.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/relay.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/kaggle_evaluation.proto\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/__init__.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n/kaggle/input/jane-street-real-time-market-data-forecasting/kaggle_evaluation/core/generated/__init__.py\n/kaggle/input/xgboost-model-json/xgb_model_2.json\n/kaggle/input/xgboost2/pytorch/default/1/xgb_model_2.json\n/kaggle/input/xgbboost/scikitlearn/default/1/xgb_model_2.json\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import polars as pl\nimport numpy as np\nimport xgboost as xgb\nimport os\nfrom kaggle_evaluation.jane_street_inference_server import JSInferenceServer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:39:19.629398Z","iopub.execute_input":"2024-11-22T15:39:19.629717Z","iopub.status.idle":"2024-11-22T15:39:19.634235Z","shell.execute_reply.started":"2024-11-22T15:39:19.629689Z","shell.execute_reply":"2024-11-22T15:39:19.633240Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Global variable to store the model\nfinal_model = None\n\ndef predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame:\n    \"\"\"Make a prediction.\"\"\"\n    global final_model\n\n    # Load the model if it's not already loaded\n    if final_model is None:\n        model_path = '/kaggle/input/xgboost-model-json/xgb_model_2.json'\n        final_model = xgb.Booster()\n        final_model.load_model(model_path)\n\n    # Get feature columns\n    feature_columns = [col for col in test.columns if col.startswith('feature_')]\n\n    # Extract features and convert to NumPy array\n    X_test = test.select(feature_columns).to_numpy()\n\n    # Create DMatrix for XGBoost\n    dtest = xgb.DMatrix(X_test)\n\n    # Predict using the loaded model\n    y_pred = final_model.predict(dtest)\n\n    # Prepare the predictions DataFrame\n    predictions = test.select('row_id').with_columns(\n        pl.Series(name='responder_6', values=y_pred)\n    )\n\n    # Ensure the DataFrame has the correct columns and types\n    assert isinstance(predictions, pl.DataFrame)\n    assert list(predictions.columns) == ['row_id', 'responder_6']\n    assert len(predictions) == len(test)\n\n    return predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:39:20.152326Z","iopub.execute_input":"2024-11-22T15:39:20.153034Z","iopub.status.idle":"2024-11-22T15:39:20.160665Z","shell.execute_reply.started":"2024-11-22T15:39:20.153004Z","shell.execute_reply":"2024-11-22T15:39:20.159757Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Set up the inference server\ninference_server = JSInferenceServer(predict)\n\n# Start the server\nif os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n    inference_server.serve()\nelse:\n    inference_server.run_local_gateway(\n        (\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n            '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n        )\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-22T15:39:20.540610Z","iopub.execute_input":"2024-11-22T15:39:20.541445Z","iopub.status.idle":"2024-11-22T15:39:21.610897Z","shell.execute_reply.started":"2024-11-22T15:39:20.541396Z","shell.execute_reply":"2024-11-22T15:39:21.610105Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}